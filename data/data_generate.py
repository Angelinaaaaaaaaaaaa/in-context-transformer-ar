# -*- coding: utf-8 -*-
"""data_generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TtwVK0FrhfVhSwwDq8KrmEiXenIKNHef
"""

import os
import sys
import torch
import argparse
import numpy as np

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from data.ar_process import generate_ar_dataset
except ImportError:
    # Fallback 如果脚本直接放在 data 目录下运行
    sys.path.append(os.path.join(current_dir, 'data'))
    from ar_process import generate_ar_dataset



def save_dataset(output_dir, filename, sequences, weights):
    path = os.path.join(output_dir, filename)
    sequences = torch.tensor(sequences, dtype=torch.float32)

    # 也保存 normalized weights
    torch.save({"sequences": sequences, "weights": weights}, path)


def main():
    parser = argparse.ArgumentParser(description="Generate AR(p) datasets for H1/H2")

    parser.add_argument('--output_dir', type=str, default='data_cache_h1')
    parser.add_argument('--p_values', type=int, nargs='+', default=[1, 2, 5, 10])
    parser.add_argument('--seeds', type=int, nargs='+', default=[0,1,2])

    parser.add_argument('--d', type=int, default=5)
    parser.add_argument('--T', type=int, default=100)
    parser.add_argument('--n_train', type=int, default=500000)
    parser.add_argument('--n_val', type=int, default=5000)
    parser.add_argument('--n_test', type=int, default=10000)

    args = parser.parse_args()

    output = os.path.join(current_dir, args.output_dir)
    os.makedirs(output, exist_ok=True)

    print("="*60)
    print("Generating AR(p) datasets")
    print("="*60)

    for p in args.p_values:
        print(f"\n### AR({p}) ###")

        for s in args.seeds:
            print(f"[seed {s}] generating train/val/test ...")

            # ========= 1. Generate base data (same_dynamics=True) =========
            train_seq, train_w = generate_ar_dataset(
                n_sequences=args.n_train, p=p, d=args.d, T=args.T,
                noise_std=1.0, same_dynamics=False, seed=s
            )
            val_seq, val_w = generate_ar_dataset(
                n_sequences=args.n_val, p=p, d=args.d, T=args.T,
                noise_std=1.0, same_dynamics=False, seed=s+1
            )
            test_seq, test_w = generate_ar_dataset(
                n_sequences=args.n_test, p=p, d=args.d, T=args.T,
                noise_std=1.0, same_dynamics=True, seed=s+2
            )

            # ========= 2. Normalization using train std =========
            scale = np.std(train_seq)
            if scale < 1e-6:
                raise ValueError("Training std too small, data degenerate!")

            train_seq /= scale
            val_seq /= scale
            test_seq /= scale

            # ========= 3. Save =========
            save_dataset(output, f"ar_p{p}_seed{s}_train.pt", train_seq, train_w)
            save_dataset(output, f"ar_p{p}_seed{s}_val.pt",   val_seq, val_w)
            save_dataset(output, f"ar_p{p}_seed{s}_test.pt",  test_seq, test_w)

    print("\nDone!")


if __name__ == "__main__":
    main()
